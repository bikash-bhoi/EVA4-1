# Week5

## Best Notebook
Session_5_3_MNIST.ipynb

## Results

Total params: 7,458

Train accuracy in last epoch: 99.33

Test accuracy in last epoch : 99.47%

Best test accuracy seen: 99.53%

## Logs of Best Notebook

0%|          | 0/938 [00:00<?, ?it/s]
Epoch: 1 Learning_Rate [0.016000000000000014]
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
Loss=0.180168017745018 Batch_id=937 Accuracy=90.99: 100%|██████████| 938/938 [00:16<00:00, 55.40it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0675, Accuracy: 9795/10000 (97.95%)

Epoch: 2 Learning_Rate [0.08828995804312317]
Loss=0.007562592625617981 Batch_id=937 Accuracy=96.43: 100%|██████████| 938/938 [00:17<00:00, 54.91it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0494, Accuracy: 9843/10000 (98.43%)

Epoch: 3 Learning_Rate [0.2507240193196124]
Loss=0.23905907571315765 Batch_id=937 Accuracy=97.42: 100%|██████████| 938/938 [00:16<00:00, 55.56it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0805, Accuracy: 9727/10000 (97.27%)

Epoch: 4 Learning_Rate [0.3809860226372645]
Loss=0.022089287638664246 Batch_id=937 Accuracy=97.94: 100%|██████████| 938/938 [00:16<00:00, 55.39it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0616, Accuracy: 9807/10000 (98.07%)

Epoch: 5 Learning_Rate [0.39776617418036475]
Loss=0.11572819948196411 Batch_id=937 Accuracy=98.29: 100%|██████████| 938/938 [00:17<00:00, 54.69it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0254, Accuracy: 9919/10000 (99.19%)

Epoch: 6 Learning_Rate [0.38019385280538953]
Loss=0.032625243067741394 Batch_id=937 Accuracy=98.49: 100%|██████████| 938/938 [00:16<00:00, 55.24it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0262, Accuracy: 9910/10000 (99.10%)

Epoch: 7 Learning_Rate [0.3466105879244678]
Loss=0.018119409680366516 Batch_id=937 Accuracy=98.59: 100%|██████████| 938/938 [00:17<00:00, 54.48it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0298, Accuracy: 9904/10000 (99.04%)

Epoch: 8 Learning_Rate [0.3000004]
Loss=0.06546550989151001 Batch_id=937 Accuracy=98.81: 100%|██████████| 938/938 [00:17<00:00, 54.47it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0289, Accuracy: 9906/10000 (99.06%)

Epoch: 9 Learning_Rate [0.24450480877451572]
Loss=0.2603624165058136 Batch_id=937 Accuracy=98.81: 100%|██████████| 938/938 [00:17<00:00, 53.89it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0243, Accuracy: 9920/10000 (99.20%)

Epoch: 10 Learning_Rate [0.18505484106679002]
Loss=0.05486692488193512 Batch_id=937 Accuracy=98.86: 100%|██████████| 938/938 [00:17<00:00, 54.11it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0211, Accuracy: 9936/10000 (99.36%)

Epoch: 11 Learning_Rate [0.12693288739954048]
Loss=0.007353842258453369 Batch_id=937 Accuracy=99.08: 100%|██████████| 938/938 [00:17<00:00, 63.99it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0220, Accuracy: 9934/10000 (99.34%)

Epoch: 12 Learning_Rate [0.0753033384200948]
Loss=0.11316913366317749 Batch_id=937 Accuracy=99.11: 100%|██████████| 938/938 [00:17<00:00, 53.89it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0201, Accuracy: 9934/10000 (99.34%)

Epoch: 13 Learning_Rate [0.03475370612782051]
Loss=0.05105774849653244 Batch_id=937 Accuracy=99.28: 100%|██████████| 938/938 [00:17<00:00, 53.94it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0161, Accuracy: 9945/10000 (99.45%)

Epoch: 14 Learning_Rate [0.008887003301016471]
Loss=0.0006534457206726074 Batch_id=937 Accuracy=99.37: 100%|██████████| 938/938 [00:17<00:00, 53.37it/s]
  0%|          | 0/938 [00:00<?, ?it/s]
Test set: Average loss: 0.0156, Accuracy: 9953/10000 (99.53%)

Epoch: 15 Learning_Rate [1.6e-06]
Loss=0.0009502619504928589 Batch_id=937 Accuracy=99.33: 100%|██████████| 938/938 [00:17<00:00, 52.98it/s]
Test set: Average loss: 0.0159, Accuracy: 9947/10000 (99.47%)
